{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "d51d34d4-55ec-4bcd-9418-4ce9cf523e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import os\n",
    "from astropy.io import fits\n",
    "import astropy.io.ascii\n",
    "from astropy.table import Table, vstack\n",
    "from astropy.visualization import SqrtStretch, simple_norm\n",
    "from astropy.convolution import convolve\n",
    "from astropy.coordinates import Angle\n",
    "from astropy.wcs import WCS\n",
    "from astropy.modeling.models import Gaussian2D\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy import units as u\n",
    "from photutils.segmentation import SourceFinder\n",
    "from photutils.segmentation import SourceCatalog\n",
    "from photutils import segmentation, morphology\n",
    "from photutils.aperture import EllipticalAperture\n",
    "from photutils.datasets import make_gaussian_sources_image\n",
    "from photutils.aperture import aperture_photometry\n",
    "from photutils.aperture import aperture_photometry\n",
    "from photutils.datasets import make_gaussian_sources_image\n",
    "from photutils.background import Background2D, MedianBackground\n",
    "from scipy.optimize import root_scalar\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "# Optional Local Files\n",
    "\n",
    "# Name of Readme file included with this software\n",
    "README = 'README'\n",
    "\n",
    "# Output Formatting\n",
    "# Directory for RHT output\n",
    "OUTPUT = '.'\n",
    "if not os.path.isdir(OUTPUT):\n",
    "    os.mkdir(OUTPUT)\n",
    "#\n",
    "\n",
    "#Initialize Variables\n",
    "\n",
    "KERNEL = 3\n",
    "SIZE = 3\n",
    "THRESH = 8\n",
    "NPIX = 20\n",
    "NLEV = 32\n",
    "CONTRAST = 0.001\n",
    "RMS = 0.004\n",
    "VERBOSE = True\n",
    "FILTER = 'F160W'\n",
    "\n",
    "def convolve_data2(data, kernel=KERNEL, size=SIZE):\n",
    "    \"\"\"\n",
    "    Convolves the input data with a 2D Gaussian kernel.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : numpy.ndarray\n",
    "        The input data (e.g., image or 2D array) that will be convolved with the kernel.\n",
    "    \n",
    "    kernel : float, optional\n",
    "        The full width at half maximum (FWHM) of the Gaussian kernel. This determines the spread of the kernel. \n",
    "        Default is a value specified by the global variable `KERNEL`.\n",
    "    \n",
    "    size : int, optional\n",
    "        The size of the Gaussian kernel (i.e., its dimensions). This will define the kernel's spatial extent. \n",
    "        Default is a value specified by the global variable `SIZE`.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    convolved_data : numpy.ndarray\n",
    "        The result of convolving the input data with the Gaussian kernel.\n",
    "    \"\"\"\n",
    "    kernel = make_2dgaussian_kernel(kernel, size=size)  # FWHM = 3.\n",
    "    convolved_data = convolve(data, kernel)\n",
    "    return convolved_data\n",
    "\n",
    "def sourceDetect(convolved_data, rms=RMS, thresh=THRESH, npix=NPIX, nlev=NLEV, contrast=CONTRAST):\n",
    "    \"\"\"\n",
    "    Detects sources in the convolved data by creating a segmentation map, deblending sources, \n",
    "    and saving the results to a FITS file and a CSV table.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    convolved_data : numpy.ndarray\n",
    "        The input convolved data (e.g., a smoothed image) in which sources will be detected.\n",
    "    \n",
    "    rms : float, optional\n",
    "        The root mean square (RMS) noise level in the image. Default is set by the global `RMS` value.\n",
    "    \n",
    "    thresh : float, optional\n",
    "        The threshold multiplier for detecting sources. It is applied to the RMS to determine the detection threshold. Default is set by the global `THRESH` value.\n",
    "    \n",
    "    npix : int, optional\n",
    "        The minimum number of pixels a detected source must span to be considered a valid detection. Default is set by the global `NPIX` value.\n",
    "    \n",
    "    nlev : int, optional\n",
    "        The number of levels used for deblending sources. Default is set by the global `NLEV` value.\n",
    "    \n",
    "    contrast : float, optional\n",
    "        The contrast parameter for deblending. Default is set by the global `CONTRAST` value.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    cat : SourceCatalog\n",
    "        A source catalog object containing the detected sources and their properties.\n",
    "    \n",
    "    tbl : Table\n",
    "        A table of source properties (e.g., centroids, flux) written to a CSV file.\n",
    "    \n",
    "    segm_deblend : numpy.ndarray\n",
    "        The deblended segmentation map.\n",
    "    \"\"\"    \n",
    "    threshold = np.mean(rms)*thresh\n",
    "    segment_map = detect_sources(convolved_data, threshold, npixels=npix)\n",
    "    #Deblend segmentation map in similar way as source extractor\n",
    "    segm_deblend = deblend_sources(convolved_data, segment_map,\n",
    "                               npixels=npix, nlevels=nlev, contrast=contrast,\n",
    "                               progress_bar=False)\n",
    "    # Create a Primary HDU object\n",
    "    segm = fits.PrimaryHDU(data=segm_deblend, header=header)\n",
    "    # Create an HDUList\n",
    "    segml = fits.HDUList([segm])\n",
    "    # Write to a new FITS file\n",
    "    output_filename = \"segmentation_map.fits\"\n",
    "    segml.writeto(output_filename, overwrite=True)\n",
    "    cat = SourceCatalog(convolved_data, segm_deblend, convolved_data=convolved_data)\n",
    "    tbl = cat.to_table()\n",
    "    tbl['xcentroid'].info.format = '.2f'  # optional format\n",
    "    tbl['ycentroid'].info.format = '.2f'\n",
    "    tbl['kron_flux'].info.format = '.2f'\n",
    "    astropy.io.ascii.write(tbl, 'Source_table.txt', format='csv', overwrite=True)\n",
    "    return cat, tbl, segm_deblend\n",
    "    \n",
    "def photutil_source(data, kernel=KERNEL, rms=RMS, size=SIZE, thresh=THRESH, npix=NPIX, nlev=NLEV, contrast=CONTRAST):\n",
    "    \"\"\"\n",
    "    Processes the input data by performing convolution and source detection. It returns the convolved data, \n",
    "    the source catalog, a table of source properties, and the deblended segmentation map.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : str or numpy.ndarray\n",
    "        The input data, either as a file path to a FITS file or as a numpy array containing the data (e.g., an image).\n",
    "    \n",
    "    kernel : float, optional\n",
    "        The full width at half maximum (FWHM) of the Gaussian kernel. Used in the convolution. Default is set by `KERNEL`.\n",
    "    \n",
    "    rms : float, optional\n",
    "        The root mean square (RMS) noise level in the image. Used for source detection. Default is set by `RMS`.\n",
    "    \n",
    "    size : int, optional\n",
    "        The size of the Gaussian kernel (its spatial extent). Default is set by `SIZE`.\n",
    "    \n",
    "    thresh : float, optional\n",
    "        The threshold multiplier for source detection. It is applied to the RMS to determine the detection threshold. Default is set by `THRESH`.\n",
    "    \n",
    "    npix : int, optional\n",
    "        The minimum number of pixels that a detected source must span to be considered valid. Default is set by `NPIX`.\n",
    "    \n",
    "    nlev : int, optional\n",
    "        The number of levels for deblending sources. Default is set by `NLEV`.\n",
    "    \n",
    "    contrast : float, optional\n",
    "        The contrast parameter used for deblending sources. Default is set by `CONTRAST`.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    convolved_data : numpy.ndarray\n",
    "        The smoothed or convolved data after applying the Gaussian kernel.\n",
    "    \n",
    "    cat : SourceCatalog\n",
    "        The source catalog containing the detected sources and their properties.\n",
    "    \n",
    "    tbl : Table\n",
    "        A table containing the properties of the detected sources, written to a CSV file.\n",
    "    \n",
    "    segm_deblend : numpy.ndarray\n",
    "        The deblended segmentation map showing source locations and contours.\n",
    "    \"\"\"\n",
    "    if isinstance(data, str):\n",
    "        data = fits.getdata(data)\n",
    "    elif not isinstance(data, np.ndarray):\n",
    "        raise TypeError('data must be str or numpy.ndarray')\n",
    "    convolved_data = convolve_data2(data, kernel=kernel, size=size)\n",
    "    cat, tbl, segm_deblend = sourceDetect(convolved_data, thresh=thresh, npix=npix, nlev=nlev, contrast=contrast)\n",
    "    \n",
    "    \n",
    "    return convolved_data, cat, tbl, segm_deblend\n",
    "\n",
    "# define the objective function to minimize\n",
    "def ellip_opt_fcn(radius, data, aperture, ba_ratio, normflux):\n",
    "    aperture.a = radius\n",
    "    aperture.b = radius * ba_ratio\n",
    "    flux, _ = aperture.do_photometry(data)\n",
    "    return flux[0] - normflux\n",
    "\n",
    "\n",
    "# define the optimizer\n",
    "def ellip_opt(data, ellip_aper, normflux, opt_fcn,\n",
    "             min_radius=0.01, max_radius=100):\n",
    "    ba_ratio = ellip_aper.b / ellip_aper.a\n",
    "    args = (data, ellip_aper, ba_ratio, normflux)\n",
    "    result = root_scalar(opt_fcn, args=args,\n",
    "                         bracket=[min_radius, max_radius],\n",
    "                         method=None)\n",
    "    return result.root\n",
    "\n",
    "def photometry_50_90(data, header, cat, tbl, segm_deblend, filter_name=FILTER, verbose=False):\n",
    "    \"\"\"\n",
    "    Performs photometric measurements on sources detected in astronomical images, calculating fluxes within elliptical \n",
    "    apertures at multiple radii (50% and 90% of the flux). It calculates the semi-major and semi-minor axes, the \n",
    "    orientations, and outputs the results in a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : str or numpy.ndarray\n",
    "        The input data, either as a file path to a FITS file or a numpy array containing the image data.\n",
    "    \n",
    "    header : astropy.io.fits.Header\n",
    "        The header of the FITS file used for WCS transformations to convert pixel coordinates to world coordinates (RA/Dec).\n",
    "    \n",
    "    cat : SourceCatalog\n",
    "        The source catalog containing the detected sources' parameters, including Kron radius, flux, and other properties.\n",
    "    \n",
    "    tbl : Table\n",
    "        The table containing the properties of detected sources, including centroid positions, orientation, and flux values.\n",
    "    \n",
    "    segm_deblend : numpy.ndarray\n",
    "        The deblended segmentation map used to identify individual sources in the image.\n",
    "    \n",
    "    filter_name : str, optional\n",
    "        The name of the filter used in the photometric observations. It is used to name output columns and files. \n",
    "        Default is set by `FILTER`.\n",
    "    \n",
    "    verbose : bool, optional\n",
    "        If True, prints status updates during the photometric measurements. Default is False.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "        The function writes the results to a CSV file, containing photometric data for each detected source.\n",
    "    \n",
    "    Notes:\n",
    "    ------\n",
    "    The CSV output includes:\n",
    "        - Source ID\n",
    "        - Pixel and world (RA/Dec) coordinates\n",
    "        - Semi-major and semi-minor axes\n",
    "        - Flux measurements at 50% and 90% radii\n",
    "        - Orientation angle\n",
    "    \"\"\"\n",
    "    semimaj = cat.kron_params[0] * cat.kron_radius.value * tbl['semimajor_sigma']\n",
    "    semimin = cat.kron_params[0] * cat.kron_radius.value * tbl['semiminor_sigma']\n",
    "    x = tbl['xcentroid']\n",
    "    y = tbl['ycentroid']\n",
    "    orientation = tbl['orientation']\n",
    "    \n",
    "    a_hl_list = np.zeros(semimaj.size)\n",
    "    b_hl_list = np.zeros(semimaj.size)\n",
    "    a_hl_list_90 = np.zeros(semimaj.size)\n",
    "    b_hl_list_90 = np.zeros(semimaj.size)\n",
    "    inner_flux = np.zeros(semimaj.size)\n",
    "    wcs = WCS(hdul[0].header)\n",
    "    data_csv = np.zeros((semimaj.size, 15))\n",
    "    \n",
    "    for i in range(0, semimaj.size):\n",
    "        mask = np.zeros(segm_deblend.data.shape, dtype=bool)\n",
    "        bad = np.where((segm_deblend.data != i + 1) | np.isnan(data))\n",
    "        ok = np.where(segm_deblend.data == 0)\n",
    "        mask[bad] = True\n",
    "        mask[ok] = False\n",
    "    \n",
    "        xypos = (x[i], y[i])\n",
    "        theta = orientation[i]\n",
    "        aper = EllipticalAperture(xypos, a=semimaj[i].value, b=semimin[i].value, theta=theta)\n",
    "        ba_ratio = aper.b / aper.a\n",
    "    \n",
    "        try:\n",
    "            max_radius_value = semimaj[i].value * 1.04\n",
    "            target_flux1 = tbl['kron_flux'][i] / 2.0  # e.g., half the flux\n",
    "            a_hl = ellip_opt(data, aper, target_flux1, ellip_opt_fcn, max_radius=max_radius_value)\n",
    "            b_hl = a_hl * ba_ratio\n",
    "            a_hl, b_hl\n",
    "        except ValueError as e:\n",
    "            if str(e) == \"f(a) and f(b) must have different signs\":\n",
    "                a_hl = 0\n",
    "                b_hl = 0\n",
    "            else:\n",
    "                raise e\n",
    "        a_hl_list[i] = a_hl\n",
    "        b_hl_list[i] = b_hl\n",
    "        if a_hl != 0:\n",
    "            aper_hl = EllipticalAperture(xypos, a=a_hl, b=b_hl, theta=theta)\n",
    "            aper_h2 = EllipticalAperture(xypos, a=semimaj[i].value, b=semimin[i].value, theta=theta)\n",
    "        \n",
    "            try:\n",
    "                max_radius_value = semimaj[i].value * 1.04\n",
    "                target_flux1 = tbl['kron_flux'][i] * 0.9  # e.g., 90% the flux\n",
    "                a_hl_90 = ellip_opt(data, aper, target_flux1, ellip_opt_fcn, max_radius=max_radius_value)\n",
    "                b_hl_90 = a_hl_90 * ba_ratio\n",
    "                a_hl_90, b_hl_90\n",
    "            except ValueError as e:\n",
    "                if str(e) == \"f(a) and f(b) must have different signs\":\n",
    "                    a_hl_90 = 0\n",
    "                    b_hl_90 = 0\n",
    "                else:\n",
    "                    raise e\n",
    "            a_hl_list_90[i] = a_hl_90\n",
    "            b_hl_list_90[i] = b_hl_90\n",
    "            if a_hl_90 != 0:\n",
    "                aper_hl_90 = EllipticalAperture(xypos, a=a_hl_90, b=b_hl_90, theta=theta)\n",
    "        \n",
    "                phot_table = aperture_photometry(data, aper_hl, method='subpixel', mask=mask, subpixels=5)\n",
    "                data_csv[i, 0] = int(phot_table['id'][0].item() + i)\n",
    "                data_csv[i, 1] = phot_table['xcenter'][0].to_value()\n",
    "                data_csv[i, 2] = phot_table['ycenter'][0].to_value()\n",
    "                coords = wcs.pixel_to_world(x, y)\n",
    "                data_csv[i, 3] = coords.ra.deg[i]\n",
    "                data_csv[i, 4] = coords.dec.deg[i]\n",
    "                data_csv[i, 5] = semimaj[i].value\n",
    "                data_csv[i, 6] = semimin[i].value\n",
    "                data_csv[i, 7] = a_hl_list[i]\n",
    "                data_csv[i, 8] = b_hl_list[i]\n",
    "                data_csv[i, 9] = a_hl_list_90[i]\n",
    "                data_csv[i, 10] = b_hl_list_90[i]\n",
    "                data_csv[i, 11] = orientation.value[i]\n",
    "                data_csv[i, 12] = phot_table['aperture_sum'][0].item()\n",
    "                phot_table = aperture_photometry(data, aper_h2, method='subpixel', mask=mask, subpixels=5)\n",
    "                data_csv[i, 13] = phot_table['aperture_sum'][0].item()\n",
    "                phot_table = aperture_photometry(data, aper_hl_90, method='subpixel', mask=mask, subpixels=5)\n",
    "                data_csv[i, 14] = phot_table['aperture_sum'][0].item()    \n",
    "            if verbose:\n",
    "                print(f\"Processed galaxy number {i}\")\n",
    "\n",
    "    inner_col = f\"{filter_name}_inner\"\n",
    "    outer_col = f\"{filter_name}_outer\"\n",
    "    ninety_col = f\"{filter_name}_90\"\n",
    "    \n",
    "    header = ['id', 'xcenter', 'ycenter', 'xworld', 'yworld', 'semimajor', 'semiminor', 'a_hl',\n",
    "              'b_hl', 'a_hl_90', 'b_hl_90', 'angle', inner_col, outer_col, ninety_col]\n",
    "\n",
    "    for row in data_csv:\n",
    "        row[0] = int(row[0])  # Ensure the first column is an integer\n",
    "\n",
    "    output_name=f\"{filter_name}_full_array_iterative.csv\"\n",
    "\n",
    "    with open(output_name, 'w', encoding='UTF8', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "\n",
    "        # Write the header\n",
    "        writer.writerow(header)\n",
    "    \n",
    "        # Write multiple rows\n",
    "        writer.writerows(data_csv)\n",
    "\n",
    "\n",
    "def GalaxyAper(data, header, kernel=KERNEL, rms=RMS, size=SIZE, thresh=THRESH, npix=NPIX, nlev=NLEV,\n",
    "               contrast=CONTRAST, filter_name=FILTER, verbose=VERBOSE):\n",
    "    \"\"\"\n",
    "    Performs source detection, convolution, and photometric measurements on astronomical images, including \n",
    "    calculating fluxes within elliptical apertures. The function also generates a convolved image and outputs \n",
    "    the photometric results to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : str or numpy.ndarray\n",
    "        The input data, either as a file path to a FITS file or a numpy array containing the image data.\n",
    "    \n",
    "    header : astropy.io.fits.Header\n",
    "        The header of the FITS file used for WCS transformations to convert pixel coordinates to world coordinates (RA/Dec).\n",
    "    \n",
    "    kernel : float, optional\n",
    "        The kernel used for convolution (e.g., Full Width at Half Maximum, FWHM). Default is set by `KERNEL`.\n",
    "    \n",
    "    rms : float, optional\n",
    "        The root mean square noise in the image. Default is set by `RMS`.\n",
    "    \n",
    "    size : int, optional\n",
    "        The size of the convolution kernel. Default is set by `SIZE`.\n",
    "    \n",
    "    thresh : float, optional\n",
    "        The threshold used for source detection. Default is set by `THRESH`.\n",
    "    \n",
    "    npix : int, optional\n",
    "        The minimum number of pixels used for source detection. Default is set by `NPIX`.\n",
    "    \n",
    "    nlev : int, optional\n",
    "        The number of levels used for deblending sources. Default is set by `NLEV`.\n",
    "    \n",
    "    contrast : float, optional\n",
    "        The contrast parameter used for deblending sources. Default is set by `CONTRAST`.\n",
    "    \n",
    "    filter_name : str, optional\n",
    "        The name of the filter used in the photometric observations. Default is set by `FILTER`.\n",
    "    \n",
    "    verbose : bool, optional\n",
    "        If True, prints status updates during the process. Default is set by `VERBOSE`.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "        The function writes the photometric results to a CSV file, containing flux measurements and source properties.\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    This function internally calls `photutil_source` for source detection and convolution, and `photometry_50_90` \n",
    "    for calculating fluxes at different radii. The results are stored in a CSV file for further analysis.\n",
    "    \"\"\"\n",
    "    if isinstance(data, str):\n",
    "        data = fits.getdata(data)\n",
    "    elif not isinstance(data, np.ndarray):\n",
    "        raise TypeError('data must be str or numpy.ndarray')\n",
    "\n",
    "    # Capture the outputs of photutil_source\n",
    "    convolved_data, cat, tbl, segm_deblend = photutil_source(\n",
    "        data, kernel=kernel, rms=rms, size=size, thresh=thresh, npix=npix, nlev=nlev, contrast=contrast\n",
    "    )\n",
    "\n",
    "    # Pass the outputs to photometry_50_90, include verbose flag\n",
    "    photometry_50_90(data, header, cat=cat, tbl=tbl, segm_deblend=segm_deblend, verbose=verbose)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def srcor(x1in, y1in, x2in, y2in, dcr=1.5, magnitude=None, spherical=2, count=False, silent=False):\n",
    "    \"\"\"\n",
    "    Match sources between two sets of coordinates (RA, DEC), using a critical distance (dcr) of 1.5 arcseconds.\n",
    "    \"\"\"\n",
    "    coords1 = SkyCoord(ra=x1in * u.deg, dec=y1in * u.deg, frame='icrs')\n",
    "    coords2 = SkyCoord(ra=x2in * u.deg, dec=y2in * u.deg, frame='icrs')\n",
    "    # Perform the matching\n",
    "    idx, sep, _ = coords1.match_to_catalog_sky(coords2)\n",
    "\n",
    "    # Ensure idx is always an array, even if there is a single match\n",
    "    if np.isscalar(idx):\n",
    "        idx = np.array([idx])\n",
    "\n",
    "    # Filter out matches that exceed the critical distance\n",
    "    matched = sep.arcsec <= dcr\n",
    "    ind1 = np.where(matched)[0]  # Indices of sources in the first list\n",
    "\n",
    "    # If there is more than one match, apply the boolean mask\n",
    "    if idx.size > 1:\n",
    "        ind2 = idx[matched]  # Indices of matched sources in the second list\n",
    "    else:\n",
    "        # If only one match, use idx directly without applying the mask\n",
    "        ind2 = idx\n",
    "\n",
    "    # If no matches were found, return empty arrays\n",
    "    if len(ind1) == 0:\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    # Return the result\n",
    "    if count:\n",
    "        return len(ind1), ind1, ind2\n",
    "    else:\n",
    "        return ind1, ind2\n",
    "\n",
    "ID_COL = 'id'\n",
    "RA_COL = 'ra'\n",
    "DEC_COL = 'dec'\n",
    "ZSPEC_COL = 'zspec'\n",
    "ZPHOT_COL = 'zfast'\n",
    "LMASS_COL = 'lmass'\n",
    "Z_LOW = 1.53\n",
    "Z_HIGH = 1.69\n",
    "\n",
    "def catalog_matching(catalog_file, GalaxyAper_output=None, id_col=ID_COL, ra_col=RA_COL, dec_col=DEC_COL, zspec_col=ZSPEC_COL,\n",
    "                   zphot_col=ZPHOT_COL, lmass_col=LMASS_COL, z_low=Z_LOW, z_high=Z_HIGH, z_choice='zspec'):\n",
    "    \"\"\"\n",
    "    Matches sources from a galaxy catalog with those detected in a photometric aperture catalog based on their \n",
    "    sky positions (RA/Dec), and writes the matched catalog entries to a new output file. The function filters \n",
    "    sources by redshift and computes the closest catalog match based on angular separation.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    catalog_file : str\n",
    "        The path to the FITS catalog file containing the galaxy data (e.g., redshift, mass, RA/Dec).\n",
    "    \n",
    "    GalaxyAper_output : str, optional\n",
    "        The path to the CSV file generated by the `GalaxyAper` function, containing aperture photometry results. \n",
    "        Default is `None`, in which case it uses a default output filename derived from `FILTER`.\n",
    "    \n",
    "    id_col : str, optional\n",
    "        The name of the ID column in the FITS catalog. Default is `ID_COL`.\n",
    "    \n",
    "    ra_col : str, optional\n",
    "        The name of the Right Ascension column in the FITS catalog. Default is `RA_COL`.\n",
    "    \n",
    "    dec_col : str, optional\n",
    "        The name of the Declination column in the FITS catalog. Default is `DEC_COL`.\n",
    "    \n",
    "    zspec_col : str, optional\n",
    "        The name of the redshift (spectroscopic) column in the FITS catalog. Default is `ZSPEC_COL`.\n",
    "    \n",
    "    zphot_col : str, optional\n",
    "        The name of the redshift (photometric) column in the FITS catalog. Default is `ZPHOT_COL`.\n",
    "    \n",
    "    lmass_col : str, optional\n",
    "        The name of the stellar mass column in the FITS catalog. Default is `LMASS_COL`.\n",
    "    \n",
    "    z_low : float, optional\n",
    "        The lower bound for redshift filtering. Default is `Z_LOW`.\n",
    "    \n",
    "    z_high : float, optional\n",
    "        The upper bound for redshift filtering. Default is `Z_HIGH`.\n",
    "    \n",
    "    z_choice : str, optional\n",
    "        The redshift source to use for filtering ('zspec', 'zphot', or 'zboth'). Default is 'zspec'. 'zboth' Considers anything\n",
    "        between the specified redshift range (zphot or zspec)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "        The function writes the matched galaxy catalog to a new file (`z_member_catalogue.cat`) with photometric \n",
    "        measurements, galaxy properties, and the closest matching catalog entries.\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    This function utilizes `srcor` for matching sources between the catalog and photometric data by their \n",
    "    RA/Dec positions. It outputs the matched entries, including flux measurements and galaxy properties, to a text \n",
    "    file suitable for further analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    # Use the default catalog_file if not provided\n",
    "    if GalaxyAper_output is None:\n",
    "        GalaxyAper_output = f\"{FILTER}_full_array_iterative.csv\"\n",
    "    \n",
    "    print(f\"Using GalaxyAper file: {GalaxyAper_output}\")\n",
    "    # Open the FITS file and read the table (assuming it's a binary table)\n",
    "    fits_file = catalog_file\n",
    "    hdulist = fits.open(fits_file)\n",
    "\n",
    "    # Extract the data from the table\n",
    "    data = hdulist[1].data  # This assumes the data is in the second HDU (index 1)\n",
    "\n",
    "    # Extract the columns by index (adjusted to column names)\n",
    "    catalog_id = data[id_col]\n",
    "    ra_ref = data[ra_col]\n",
    "    dec_ref = data[dec_col]\n",
    "    zspec = data[zspec_col] if zspec_col in data.dtype.names else None\n",
    "    zphot = data[zphot_col] if zphot_col in data.dtype.names else None\n",
    "    lmass = data[lmass_col] if lmass_col in data.dtype.names else None\n",
    "\n",
    "    hdulist.close()\n",
    "\n",
    "    # Now read the CSV for additional data\n",
    "    csv_data = np.genfromtxt(GalaxyAper_output, delimiter=',', skip_header=1)\n",
    "\n",
    "    # Extract the columns from the CSV file\n",
    "    id1 = csv_data[:, 0].astype(int)\n",
    "    xcenter = csv_data[:, 1]\n",
    "    ycenter = csv_data[:, 2]\n",
    "    RA = csv_data[:, 3]\n",
    "    DEC = csv_data[:, 4]\n",
    "    semimaj = csv_data[:, 5]\n",
    "    semimin = csv_data[:, 6]\n",
    "    a_hl = csv_data[:, 7]\n",
    "    b_hl = csv_data[:, 8]\n",
    "    a_hl_90 = csv_data[:, 9]\n",
    "    b_hl_90 = csv_data[:, 10]\n",
    "    angle = csv_data[:, 11]\n",
    "    F160W_inner = csv_data[:, 12]\n",
    "    F160W_outer = csv_data[:, 13]\n",
    "    F160W_90 = csv_data[:, 14]\n",
    "\n",
    "    # Define critical distance (1.5 arcseconds)\n",
    "    dcr = 1.5  # arcseconds\n",
    "\n",
    "    # Filter data for zspec between z_low and z_high\n",
    "    # Apply the selection based on z_choice\n",
    "    if z_choice == 'zspec' and zspec is not None:\n",
    "        good = np.where((zspec > z_low) & (zspec < z_high))[0]\n",
    "    elif z_choice == 'zphot' and zphot is not None:\n",
    "        good = np.where((zphot > z_low) & (zphot < z_high))[0]\n",
    "    elif z_choice == 'zboth':\n",
    "        good_zspec = np.where((zspec > z_low) & (zspec < z_high))[0] if zspec is not None else []\n",
    "        good_zphot = np.where((zphot > z_low) & (zphot < z_high))[0] if zphot is not None else []\n",
    "        \n",
    "        # Only concatenate if both are available\n",
    "        good = np.unique(np.concatenate((good_zspec, good_zphot)))  # Combine and remove duplicates\n",
    "    else:\n",
    "        raise ValueError(\"Invalid value for z_choice or missing redshift data. Ensure zspec or zphot are available.\")\n",
    "\n",
    "    # Filter catalog data based on the condition\n",
    "    catalog_id = catalog_id[good]\n",
    "    ra_ref = ra_ref[good]\n",
    "    dec_ref = dec_ref[good]\n",
    "    zspec = zspec[good]\n",
    "    zfast = zphot[good]\n",
    "    lmass = lmass[good]\n",
    "\n",
    "    inner_col = f\"{filter_name}_inner\"\n",
    "    outer_col = f\"{filter_name}_outer\"\n",
    "    ninety_col = f\"{filter_name}_90\"\n",
    "    \n",
    "    # Create the output catalog file\n",
    "    with open('z_member_catalogue.cat', 'w') as f:\n",
    "        header = f\"# old_id ID xcen ycen RA DEC semimaj semimin a_hl b_hl a_hl_90 b_hl_90 angle {inner_col} {outer_col} {ninety_col} lmass\\n\"\n",
    "        f.write(header)\n",
    "        # Prepare the data for output\n",
    "        phdata = np.array([F160W_inner, F160W_outer, F160W_90])\n",
    "\n",
    "        # Loop over the data and find the closest catalog source using srcor\n",
    "        for i in range(len(ra_ref)):\n",
    "            #print(i)\n",
    "            # Use srcor to find the closest catalog entry to the current source (RA[i], DEC[i])\n",
    "            ind1, ind2 = srcor(ra_ref[i], dec_ref[i], RA, DEC, dcr=dcr)\n",
    "\n",
    "            # If no match is found, continue to the next iteration\n",
    "            if len(ind1) == 0:\n",
    "                continue\n",
    "\n",
    "            # Compute angular distances for all matches\n",
    "            if len(ind1) > 1:\n",
    "                dist = np.sqrt((RA[ind2] - ra_ref[i])**2 + (DEC[ind2] - dec_ref[i])**2)\n",
    "                closest_index = ind2[np.argmin(dist)]  # Choose the closest match\n",
    "\n",
    "                f.write(f\"{id1[ind2]} {catalog_id[i]} {xcenter[closest_index]:.4f} {ycenter[closest_index]:.4f} \"\n",
    "                        f\"{RA[closest_index]:.6f} {DEC[closest_index]:.6f} {semimaj[closest_index]:.4f} {semimin[closest_index]:.4f} {a_hl[closest_index]:.4f} \"\n",
    "                        f\"{b_hl[closest_index]:.4f} {a_hl_90[closest_index]:.4f} {b_hl_90[closest_index]:.4f} {angle[closest_index]:.4f} \"\n",
    "                        f\"{phdata[0, closest_index]:.4f} {phdata[1, closest_index]:.4f} {phdata[2, closest_index]:.4f} {lmass[i]:.4f}\\n\")\n",
    "\n",
    "            if len(ind1) == 1:\n",
    "                f.write(f\"{id1[ind2]} {catalog_id[i]} {xcenter[ind2]:.4f} {ycenter[ind2]:.4f} \"\n",
    "                        f\"{RA[ind2]:.6f} {DEC[ind2]:.6f} {semimaj[ind2]:.4f} {semimin[ind2]:.4f} {a_hl[ind2]:.4f} \"\n",
    "                        f\"{b_hl[ind2]:.4f} {a_hl_90[ind2]:.4f} {b_hl_90[ind2]:.4f} {angle[ind2]:.4f} \"\n",
    "                        f\"{phdata[0, ind2]:.4f} {phdata[1, ind2]:.4f} {phdata[2, ind2]:.4f} {lmass[i]:.4f}\\n\")\n",
    "\n",
    "    print(\"Output written to z_member_catalogue.cat\")\n",
    "\n",
    "def GalaxyAper_multi_filter(data, filter_data_map, catalog_matching_output=None, filter_name=FILTER):\n",
    "    \"\"\"\n",
    "    Processes aperture photometry for multiple filters and saves the results into a CSV file. For each filter, it \n",
    "    computes aperture photometry on the galaxy positions from a catalog and stores the photometric measurements for \n",
    "    inner, outer, and r90 apertures.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : str or numpy.ndarray\n",
    "        The input FITS data for a single filter. If a string is provided, it is assumed to be a file path to a FITS file. \n",
    "        If a numpy array is provided, it represents the data array for the filter.\n",
    "    \n",
    "    filter_data_map : dict\n",
    "        A dictionary where keys are filter names (e.g., 'F625W') and values are the corresponding FITS data arrays for \n",
    "        each filter.\n",
    "    \n",
    "    catalog_matching_output : str, optional\n",
    "        The path to the catalog file containing source positions. Defaults to 'z_member_catalogue.cat'.\n",
    "    \n",
    "    filter_name : str, optional\n",
    "        The name of the filter used for catalog matching, included in the column names for the output. Defaults to `FILTER`.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "        The function writes the aperture photometry results for all filters to a CSV file (`full_array_iterative_all_filters.csv`).\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    - The function assumes that the catalog data is available and processed using a segmentation map (`segmentation_map.fits`).\n",
    "    - Aperture photometry is performed using three different aperture sizes: inner, outer, and r90.\n",
    "    - The results for all filters are compiled into a single CSV file with columns for the photometric measurements across all filters.\n",
    "    \"\"\"\n",
    "    if catalog_matching_output is None:\n",
    "        catalog_matching_output = 'z_member_catalogue.cat'\n",
    "    \n",
    "    print(f\"Using catalog_matching file: {catalog_matching_output}\")\n",
    "    table = ascii.read(catalog_matching_output)\n",
    "    num_filters = len(filter_data_map)\n",
    "    num_columns_per_filter = 3  # inner, outer, 90\n",
    "    base_columns = 16  # ID, position, etc.\n",
    "\n",
    "    data_csv = np.zeros((len(table), base_columns + num_filters * num_columns_per_filter))\n",
    "\n",
    "    inner_col = f\"{filter_name}_inner\"\n",
    "    outer_col = f\"{filter_name}_outer\"\n",
    "    ninety_col = f\"{filter_name}_90\"\n",
    "    \n",
    "    # Fill the base columns\n",
    "    for i in range(len(table)):\n",
    "        data_csv[i, :16] = [\n",
    "            table['ID'][i], table['xcen'][i], table['ycen'][i], table['RA'][i], table['DEC'][i],\n",
    "            table['semimaj'][i], table['semimin'][i], table['a_hl'][i], table['b_hl'][i],\n",
    "            table['a_hl_90'][i], table['b_hl_90'][i], table['angle'][i], table['lmass'][i],\n",
    "            table[inner_col][i], table[outer_col][i], table[ninety_col][i]\n",
    "        ]\n",
    "\n",
    "    segm_deblend = fits.open('segmentation_map.fits')\n",
    "    \n",
    "    # Process each filter\n",
    "    filter_start_index = base_columns\n",
    "    for filter_name, data in filter_data_map.items():\n",
    "        print(f\"Processing filter: {filter_name}\")\n",
    "        for i in range(len(table)):\n",
    "            mask = np.zeros(data.shape, dtype=bool)\n",
    "            bad = np.where(segm_deblend[0].data != table['old_id'][i])\n",
    "            ok = np.where(segm_deblend[0].data == 0)\n",
    "            mask[bad] = True\n",
    "            mask[ok] = False\n",
    "\n",
    "            xypos = (table['xcen'][i], table['ycen'][i])\n",
    "            theta = np.deg2rad(table['angle'][i])\n",
    "\n",
    "            # Perform aperture photometry for inner, outer, and 90 apertures\n",
    "            apertures = [\n",
    "                EllipticalAperture(xypos, a=table['a_hl'][i], b=table['b_hl'][i], theta=theta),\n",
    "                EllipticalAperture(xypos, a=table['semimaj'][i], b=table['semimin'][i], theta=theta),\n",
    "                EllipticalAperture(xypos, a=table['a_hl_90'][i], b=table['b_hl_90'][i], theta=theta)\n",
    "            ]\n",
    "            \n",
    "            for j, aper in enumerate(apertures):\n",
    "                phot_table = aperture_photometry(data, aper, method='subpixel', mask=mask, subpixels=5)\n",
    "                data_csv[i, filter_start_index + j] = phot_table['aperture_sum'][0].item()\n",
    "\n",
    "        filter_start_index += num_columns_per_filter\n",
    "\n",
    "    # Generate dynamic header\n",
    "    header = ['id', 'xcenter', 'ycenter', 'xworld', 'yworld', 'semimajor', 'semiminor', 'a_hl',\n",
    "              'b_hl', 'a_hl_90', 'b_hl_90', 'angle', 'lmass', inner_col, outer_col, ninety_col]\n",
    "    for filter_name in filter_data_map.keys():\n",
    "        header.extend([f\"{filter_name}_inner\", f\"{filter_name}_outer\", f\"{filter_name}_90\"])\n",
    "\n",
    "    # Save the results\n",
    "    output_file = 'full_array_iterative_all_filters.csv'\n",
    "    with open(output_file, 'w', encoding='UTF8', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(header)\n",
    "        writer.writerows(data_csv)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "5be714d4-f5a9-4aab-8fb8-5168d6ec8e04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed galaxy number 0\n",
      "Processed galaxy number 1\n",
      "Processed galaxy number 2\n",
      "Processed galaxy number 3\n",
      "Processed galaxy number 4\n",
      "Processed galaxy number 5\n",
      "Processed galaxy number 6\n",
      "Processed galaxy number 7\n",
      "Processed galaxy number 8\n",
      "Processed galaxy number 9\n",
      "Processed galaxy number 10\n",
      "Processed galaxy number 11\n",
      "Processed galaxy number 12\n",
      "Processed galaxy number 13\n",
      "Processed galaxy number 14\n",
      "Processed galaxy number 15\n",
      "Processed galaxy number 16\n",
      "Processed galaxy number 17\n",
      "Processed galaxy number 18\n",
      "Processed galaxy number 19\n",
      "Processed galaxy number 20\n",
      "Processed galaxy number 21\n",
      "Processed galaxy number 22\n",
      "Processed galaxy number 23\n",
      "Processed galaxy number 24\n",
      "Processed galaxy number 25\n",
      "Processed galaxy number 26\n",
      "Processed galaxy number 27\n",
      "Processed galaxy number 28\n",
      "Processed galaxy number 29\n",
      "Processed galaxy number 30\n",
      "Processed galaxy number 31\n",
      "Processed galaxy number 32\n",
      "Processed galaxy number 33\n",
      "Processed galaxy number 34\n",
      "Processed galaxy number 35\n",
      "Processed galaxy number 36\n",
      "Processed galaxy number 37\n",
      "Processed galaxy number 38\n",
      "Processed galaxy number 39\n",
      "Processed galaxy number 40\n",
      "Processed galaxy number 41\n",
      "Processed galaxy number 42\n",
      "Processed galaxy number 43\n",
      "Processed galaxy number 44\n",
      "Processed galaxy number 45\n",
      "Processed galaxy number 46\n",
      "Processed galaxy number 47\n",
      "Processed galaxy number 48\n",
      "Processed galaxy number 49\n",
      "Processed galaxy number 50\n",
      "Processed galaxy number 51\n",
      "Processed galaxy number 52\n",
      "Processed galaxy number 53\n",
      "Processed galaxy number 54\n",
      "Processed galaxy number 55\n",
      "Processed galaxy number 56\n",
      "Processed galaxy number 57\n",
      "Processed galaxy number 58\n",
      "Processed galaxy number 59\n",
      "Processed galaxy number 60\n",
      "Processed galaxy number 61\n",
      "Processed galaxy number 62\n",
      "Processed galaxy number 63\n",
      "Processed galaxy number 64\n",
      "Processed galaxy number 65\n",
      "Processed galaxy number 66\n",
      "Processed galaxy number 67\n",
      "Processed galaxy number 68\n",
      "Processed galaxy number 69\n",
      "Processed galaxy number 70\n",
      "Processed galaxy number 71\n",
      "Processed galaxy number 72\n",
      "Processed galaxy number 73\n",
      "Processed galaxy number 74\n",
      "Processed galaxy number 75\n",
      "Processed galaxy number 76\n",
      "Processed galaxy number 77\n",
      "Processed galaxy number 78\n",
      "Processed galaxy number 79\n",
      "Processed galaxy number 80\n",
      "Processed galaxy number 81\n",
      "Processed galaxy number 82\n",
      "Processed galaxy number 83\n",
      "Processed galaxy number 84\n",
      "Processed galaxy number 85\n",
      "Processed galaxy number 86\n",
      "Processed galaxy number 87\n",
      "Processed galaxy number 88\n",
      "Processed galaxy number 89\n",
      "Processed galaxy number 90\n",
      "Processed galaxy number 91\n",
      "Processed galaxy number 92\n",
      "Processed galaxy number 93\n",
      "Processed galaxy number 94\n",
      "Processed galaxy number 95\n",
      "Processed galaxy number 96\n",
      "Processed galaxy number 97\n",
      "Processed galaxy number 98\n",
      "Processed galaxy number 99\n",
      "Processed galaxy number 100\n",
      "Processed galaxy number 101\n",
      "Processed galaxy number 102\n",
      "Processed galaxy number 103\n",
      "Processed galaxy number 104\n",
      "Processed galaxy number 105\n",
      "Processed galaxy number 106\n",
      "Processed galaxy number 107\n",
      "Processed galaxy number 108\n",
      "Processed galaxy number 109\n",
      "Processed galaxy number 110\n",
      "Processed galaxy number 111\n",
      "Processed galaxy number 112\n",
      "Processed galaxy number 113\n",
      "Processed galaxy number 114\n",
      "Processed galaxy number 115\n",
      "Processed galaxy number 116\n",
      "Processed galaxy number 117\n",
      "Processed galaxy number 118\n",
      "Processed galaxy number 119\n",
      "Processed galaxy number 120\n",
      "Processed galaxy number 121\n",
      "Processed galaxy number 122\n",
      "Processed galaxy number 123\n",
      "Processed galaxy number 124\n",
      "Processed galaxy number 125\n",
      "Processed galaxy number 126\n",
      "Processed galaxy number 127\n",
      "Processed galaxy number 128\n",
      "Processed galaxy number 129\n",
      "Processed galaxy number 130\n",
      "Processed galaxy number 131\n",
      "Processed galaxy number 132\n",
      "Processed galaxy number 133\n",
      "Processed galaxy number 134\n",
      "Processed galaxy number 135\n",
      "Processed galaxy number 136\n",
      "Processed galaxy number 137\n",
      "Processed galaxy number 138\n",
      "Processed galaxy number 139\n",
      "Processed galaxy number 140\n",
      "Processed galaxy number 141\n",
      "Processed galaxy number 142\n",
      "Processed galaxy number 143\n",
      "Processed galaxy number 144\n",
      "Processed galaxy number 145\n",
      "Processed galaxy number 146\n",
      "Processed galaxy number 147\n",
      "Processed galaxy number 148\n",
      "Processed galaxy number 149\n",
      "Processed galaxy number 150\n",
      "Processed galaxy number 151\n",
      "Processed galaxy number 152\n",
      "Processed galaxy number 153\n",
      "Processed galaxy number 154\n",
      "Processed galaxy number 155\n",
      "Processed galaxy number 156\n",
      "Processed galaxy number 157\n",
      "Processed galaxy number 158\n",
      "Processed galaxy number 159\n",
      "Processed galaxy number 160\n",
      "Processed galaxy number 161\n",
      "Processed galaxy number 162\n",
      "Processed galaxy number 163\n",
      "Processed galaxy number 164\n",
      "Processed galaxy number 165\n",
      "Processed galaxy number 166\n",
      "Processed galaxy number 167\n",
      "Processed galaxy number 168\n",
      "Processed galaxy number 169\n",
      "Processed galaxy number 170\n",
      "Processed galaxy number 171\n",
      "Processed galaxy number 172\n",
      "Processed galaxy number 173\n",
      "Processed galaxy number 174\n",
      "Processed galaxy number 175\n",
      "Processed galaxy number 176\n",
      "Processed galaxy number 177\n",
      "Processed galaxy number 178\n",
      "Processed galaxy number 179\n",
      "Processed galaxy number 180\n",
      "Processed galaxy number 181\n",
      "Processed galaxy number 182\n",
      "Processed galaxy number 183\n",
      "Processed galaxy number 184\n",
      "Processed galaxy number 185\n",
      "Processed galaxy number 186\n",
      "Processed galaxy number 187\n",
      "Processed galaxy number 188\n",
      "Processed galaxy number 189\n",
      "Processed galaxy number 190\n",
      "Processed galaxy number 191\n",
      "Processed galaxy number 192\n",
      "Processed galaxy number 193\n",
      "Processed galaxy number 194\n",
      "Processed galaxy number 195\n",
      "Processed galaxy number 196\n",
      "Processed galaxy number 197\n",
      "Processed galaxy number 198\n",
      "Processed galaxy number 199\n",
      "Processed galaxy number 200\n",
      "Processed galaxy number 201\n",
      "Processed galaxy number 202\n",
      "Processed galaxy number 203\n",
      "Processed galaxy number 204\n",
      "Processed galaxy number 205\n",
      "Processed galaxy number 206\n",
      "Processed galaxy number 207\n",
      "Processed galaxy number 208\n",
      "Processed galaxy number 209\n",
      "Processed galaxy number 210\n",
      "Processed galaxy number 211\n",
      "Processed galaxy number 212\n",
      "Processed galaxy number 213\n",
      "Processed galaxy number 214\n",
      "Processed galaxy number 215\n",
      "Processed galaxy number 216\n",
      "Processed galaxy number 217\n",
      "Processed galaxy number 218\n",
      "Processed galaxy number 219\n",
      "Processed galaxy number 220\n",
      "Processed galaxy number 221\n",
      "Processed galaxy number 222\n",
      "Processed galaxy number 223\n",
      "Processed galaxy number 224\n",
      "Processed galaxy number 225\n",
      "Processed galaxy number 226\n",
      "Processed galaxy number 227\n",
      "Processed galaxy number 228\n",
      "Processed galaxy number 229\n",
      "Processed galaxy number 230\n",
      "Processed galaxy number 231\n",
      "Processed galaxy number 232\n",
      "Processed galaxy number 233\n",
      "Processed galaxy number 234\n",
      "Processed galaxy number 235\n",
      "Processed galaxy number 236\n",
      "Processed galaxy number 237\n",
      "Processed galaxy number 238\n",
      "Processed galaxy number 239\n",
      "Processed galaxy number 240\n",
      "Processed galaxy number 241\n",
      "Processed galaxy number 242\n",
      "Processed galaxy number 243\n",
      "Processed galaxy number 244\n",
      "Processed galaxy number 245\n",
      "Processed galaxy number 246\n",
      "Processed galaxy number 247\n",
      "Processed galaxy number 248\n",
      "Processed galaxy number 249\n",
      "Processed galaxy number 250\n",
      "Processed galaxy number 251\n",
      "Processed galaxy number 252\n",
      "Processed galaxy number 253\n",
      "Processed galaxy number 254\n",
      "Processed galaxy number 255\n",
      "Processed galaxy number 256\n",
      "Processed galaxy number 257\n",
      "Processed galaxy number 258\n",
      "Processed galaxy number 259\n",
      "Processed galaxy number 260\n",
      "Processed galaxy number 261\n",
      "Processed galaxy number 262\n",
      "Processed galaxy number 263\n",
      "Processed galaxy number 264\n",
      "Processed galaxy number 265\n",
      "Processed galaxy number 266\n",
      "Processed galaxy number 267\n",
      "Processed galaxy number 268\n",
      "Processed galaxy number 269\n",
      "Processed galaxy number 270\n",
      "Processed galaxy number 271\n",
      "Processed galaxy number 272\n",
      "Processed galaxy number 273\n",
      "Processed galaxy number 274\n",
      "Processed galaxy number 275\n",
      "Processed galaxy number 276\n",
      "Processed galaxy number 277\n",
      "Processed galaxy number 278\n",
      "Processed galaxy number 279\n",
      "Processed galaxy number 280\n",
      "Processed galaxy number 281\n",
      "Processed galaxy number 282\n",
      "Processed galaxy number 283\n",
      "Processed galaxy number 284\n",
      "Processed galaxy number 285\n",
      "Processed galaxy number 286\n",
      "Processed galaxy number 287\n",
      "Processed galaxy number 288\n",
      "Processed galaxy number 289\n",
      "Processed galaxy number 290\n",
      "Processed galaxy number 291\n",
      "Processed galaxy number 292\n",
      "Processed galaxy number 293\n",
      "Processed galaxy number 294\n",
      "Processed galaxy number 295\n",
      "Processed galaxy number 296\n",
      "Processed galaxy number 297\n",
      "Processed galaxy number 298\n",
      "Processed galaxy number 299\n",
      "Processed galaxy number 300\n",
      "Processed galaxy number 301\n",
      "Processed galaxy number 302\n",
      "Processed galaxy number 303\n",
      "Processed galaxy number 304\n",
      "Processed galaxy number 305\n",
      "Processed galaxy number 306\n",
      "Processed galaxy number 307\n",
      "Processed galaxy number 308\n",
      "Processed galaxy number 309\n",
      "Processed galaxy number 310\n",
      "Processed galaxy number 311\n",
      "Processed galaxy number 312\n",
      "Processed galaxy number 313\n",
      "Processed galaxy number 314\n",
      "Processed galaxy number 315\n",
      "Processed galaxy number 316\n",
      "Processed galaxy number 317\n",
      "Processed galaxy number 318\n",
      "Processed galaxy number 319\n",
      "Processed galaxy number 320\n",
      "Processed galaxy number 321\n",
      "Processed galaxy number 322\n",
      "Processed galaxy number 323\n",
      "Processed galaxy number 324\n",
      "Processed galaxy number 325\n",
      "Processed galaxy number 326\n",
      "Processed galaxy number 327\n",
      "Processed galaxy number 328\n",
      "Processed galaxy number 329\n",
      "Processed galaxy number 330\n",
      "Processed galaxy number 331\n",
      "Processed galaxy number 332\n",
      "Processed galaxy number 333\n",
      "Processed galaxy number 334\n",
      "Processed galaxy number 335\n",
      "Processed galaxy number 336\n",
      "Processed galaxy number 337\n",
      "Processed galaxy number 338\n",
      "Processed galaxy number 339\n",
      "Processed galaxy number 340\n",
      "Processed galaxy number 341\n",
      "Processed galaxy number 342\n",
      "Processed galaxy number 343\n",
      "Processed galaxy number 344\n",
      "Processed galaxy number 345\n",
      "Processed galaxy number 346\n",
      "Processed galaxy number 347\n",
      "Processed galaxy number 348\n",
      "Processed galaxy number 349\n",
      "Processed galaxy number 350\n",
      "Processed galaxy number 351\n",
      "Processed galaxy number 352\n",
      "Processed galaxy number 353\n",
      "Processed galaxy number 354\n",
      "Processed galaxy number 355\n",
      "Processed galaxy number 356\n",
      "Processed galaxy number 357\n",
      "Processed galaxy number 358\n",
      "Processed galaxy number 359\n",
      "Processed galaxy number 360\n",
      "Processed galaxy number 361\n",
      "Processed galaxy number 362\n",
      "Processed galaxy number 363\n",
      "Processed galaxy number 364\n",
      "Processed galaxy number 365\n",
      "Processed galaxy number 366\n",
      "Processed galaxy number 367\n",
      "Processed galaxy number 368\n",
      "Processed galaxy number 369\n",
      "Processed galaxy number 370\n",
      "Processed galaxy number 371\n",
      "Processed galaxy number 372\n",
      "Processed galaxy number 373\n",
      "Processed galaxy number 374\n",
      "Processed galaxy number 375\n",
      "Processed galaxy number 376\n",
      "Processed galaxy number 377\n",
      "Processed galaxy number 378\n",
      "Processed galaxy number 379\n",
      "Processed galaxy number 380\n",
      "Processed galaxy number 381\n",
      "Processed galaxy number 382\n",
      "Processed galaxy number 383\n",
      "Processed galaxy number 384\n",
      "Processed galaxy number 385\n",
      "Processed galaxy number 386\n",
      "Processed galaxy number 387\n",
      "Processed galaxy number 388\n",
      "Processed galaxy number 389\n",
      "Processed galaxy number 390\n",
      "Processed galaxy number 391\n",
      "Processed galaxy number 392\n",
      "Processed galaxy number 393\n",
      "Processed galaxy number 394\n",
      "Processed galaxy number 395\n",
      "Processed galaxy number 396\n",
      "Processed galaxy number 397\n",
      "Processed galaxy number 398\n",
      "Processed galaxy number 399\n",
      "Processed galaxy number 400\n",
      "Processed galaxy number 401\n",
      "Processed galaxy number 402\n",
      "Processed galaxy number 403\n",
      "Processed galaxy number 404\n",
      "Processed galaxy number 405\n",
      "Processed galaxy number 406\n",
      "Processed galaxy number 407\n",
      "Processed galaxy number 408\n",
      "Processed galaxy number 409\n",
      "Processed galaxy number 410\n",
      "Processed galaxy number 411\n",
      "Processed galaxy number 412\n",
      "Processed galaxy number 413\n",
      "Processed galaxy number 414\n",
      "Processed galaxy number 415\n",
      "Processed galaxy number 416\n",
      "Processed galaxy number 417\n",
      "Processed galaxy number 418\n",
      "Processed galaxy number 419\n",
      "Processed galaxy number 420\n",
      "Processed galaxy number 421\n",
      "Processed galaxy number 422\n",
      "Processed galaxy number 423\n",
      "Processed galaxy number 424\n",
      "Processed galaxy number 425\n",
      "Processed galaxy number 426\n",
      "Processed galaxy number 427\n",
      "Processed galaxy number 428\n",
      "Processed galaxy number 429\n",
      "Processed galaxy number 430\n",
      "Processed galaxy number 431\n",
      "Processed galaxy number 432\n",
      "Processed galaxy number 433\n",
      "Processed galaxy number 434\n",
      "Processed galaxy number 435\n",
      "Processed galaxy number 436\n",
      "Processed galaxy number 437\n",
      "Processed galaxy number 438\n",
      "Processed galaxy number 439\n",
      "Processed galaxy number 440\n",
      "Processed galaxy number 441\n",
      "Processed galaxy number 442\n",
      "Processed galaxy number 443\n",
      "Processed galaxy number 444\n",
      "Processed galaxy number 445\n",
      "Processed galaxy number 446\n",
      "Processed galaxy number 447\n",
      "Processed galaxy number 448\n",
      "Processed galaxy number 449\n",
      "Processed galaxy number 450\n",
      "Processed galaxy number 451\n",
      "Processed galaxy number 452\n",
      "Processed galaxy number 453\n",
      "Processed galaxy number 454\n",
      "Processed galaxy number 455\n",
      "Processed galaxy number 456\n",
      "Processed galaxy number 457\n",
      "Processed galaxy number 458\n",
      "Processed galaxy number 459\n",
      "Processed galaxy number 460\n",
      "Processed galaxy number 461\n",
      "Processed galaxy number 462\n",
      "Processed galaxy number 463\n",
      "Processed galaxy number 464\n",
      "Processed galaxy number 465\n",
      "Processed galaxy number 466\n",
      "Processed galaxy number 467\n",
      "Processed galaxy number 468\n",
      "Processed galaxy number 469\n",
      "Processed galaxy number 470\n",
      "Processed galaxy number 471\n",
      "Processed galaxy number 472\n",
      "Processed galaxy number 473\n",
      "Processed galaxy number 474\n",
      "Processed galaxy number 475\n",
      "Processed galaxy number 476\n",
      "Processed galaxy number 477\n",
      "Processed galaxy number 478\n",
      "Processed galaxy number 479\n",
      "Processed galaxy number 480\n",
      "Processed galaxy number 481\n",
      "Processed galaxy number 482\n",
      "Processed galaxy number 483\n",
      "Processed galaxy number 484\n",
      "Processed galaxy number 485\n",
      "Processed galaxy number 486\n",
      "Processed galaxy number 487\n",
      "Processed galaxy number 488\n",
      "Processed galaxy number 489\n",
      "Processed galaxy number 490\n",
      "Processed galaxy number 491\n",
      "Processed galaxy number 492\n",
      "Processed galaxy number 493\n"
     ]
    }
   ],
   "source": [
    "hdul = fits.open('J0224_f160w_drz.0001.resamp.fits')\n",
    "data = hdul[0].data\n",
    "header= hdul[0].header\n",
    "filter_name='F160W'\n",
    "\n",
    "\n",
    "GalaxyAper(data, header, filter_name=filter_name, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "eb1ae022-8168-4b70-be01-9730d9329a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GalaxyAper file: F160W_full_array_iterative.csv\n",
      "Output written to z_member_catalogue.cat\n"
     ]
    }
   ],
   "source": [
    "catalog_file='xmm113_catalog_v7.fits'\n",
    "catalog_matching(catalog_file, z_choice='zboth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "005f5ba8-aea2-42a7-8e43-39476c518151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using catalog_matching file: z_member_catalogue.cat\n",
      "Processing filter: F625W\n",
      "Processing filter: F475W\n"
     ]
    }
   ],
   "source": [
    "# Load FITS files into memory\n",
    "filter_data_map = {\n",
    "    'F625W': fits.open('J0224_f625w_drc.0001.resamp.fits')[0].data,\n",
    "    'F475W': fits.open('J0224_f475w_drc.0001.resamp.fits')[0].data,\n",
    "    # Add more filters as needed\n",
    "}\n",
    "\n",
    "# Call the function\n",
    "GalaxyAper_multi_filter(data, filter_data_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa500c6f-5312-4e87-a8de-0d6e0d0e1f5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
